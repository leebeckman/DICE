email notes

http://thesiswhisperer.com/2011/03/24/how-to-write-1000-words-a-day-and-not-go-bat-shit-crazy/

Control flow is not enough for post computation, as we cannot see if meaningful data is actually being output.

By seeing where communicated data is actually unpacked from complex objects and used, we can determine in some cases that data is passed but never used, which would change communication cost estimates.

Maybe the intense clone nop[0] errors are caused when I supply the additional jars (for missing library types), but give the wrong version of the library?

After looking at it more, I believe I can run the analysis for precomputation and caching the same. The only difference really is what the developer would have to do afterwards, given what they were told about the variability of the data in question. I don't think even invalidation means anything special. In fluxo, a cache only has invalidation nodes if the cache computation uses data tables in addition to the main lookup value - when these tables change the computations based on them must be invalidated. Really this just seems like another input to the cached nodes, and I don't see much reason to treat it differently than the lookup value input. I'm modifying the precomputation/caching analysis with these things in mind.

http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iyengar:Arun.html (paper about smart caching in here somewhere, possibly around 2005)

If user input drive a database selection, the result of that selection should also be tainted by the input

Could write a correlator aspect which tries to determine if certain inputs indirectly influence the behaviour of methods

------------

Hello Rodger,

You did not. I missed your previous request for an update. 

It's generally going well over here. Having finally made it through the engineering challenges, I'm now performing evaluation. 

One of the big diversions I had to take care of was rearchitecting the tracker to improve performance. The object deep scanning strategy ended up with over hour-long page loads on JGossip, once some necessary instrumentation had been added. I modified the tracker to use a more complex strategy which essentially keeps track of all object references so that taint can be propagated to everything it affects whenever a field changes. This way most of the "is this complex object tainted" checks happen immediately without requiring any deep scanning. In the end this worked well, and performance is now acceptable. I'd estimate JGossip page loads take around 30 seconds to a minute.

For evaluation I've been modifying RUBiS, gathering data, and working on visualizing meaningfully/analyzing that data. I've been able to locate persistent state in RUBiS (which I had to add in the first place) coming from both database data and user request data, and I also have results for caching/pre and post computation. I have not yet done state relocation to the user, or anything with access path refactoring. I'm going to be working on these a bit over the long weekend. Generally, this work is going much faster than building the tracker itself, since the data from the tracker is very thorough and pretty easy to work with.

I'd estimate I'm about a week and a half behind schedule, mainly because of the performance problems mentioned, so I'm putting in the time to catch up.

How's it going in Europe?

------------

Hello Eric and Rodger,

I was wondering if it would be okay to postpone the meeting today? I'm still working on visualization, which we've already discussed, but am not at a stage where I have anything interesting to share beyond a lot of engineering. I intend to transition from the visuals to the preparation of the slides we discussed, which will lay the groundwork for the workshop paper. This will have to happen very soon, considering that my presentation is at the end of next week. At this point I'd really rather spend the transit/meeting time today continuing on the work.

Update on what I was up to last week:
-Worked on visualization, got to the point where I was able to use JUNG to produce some representations of the data I was collecting.
-This revealed the fact that I needed to collect some additional data for completeness of tracking.
-This further revealed that AspectJ's inability to handle array element access would be a problem.
-Considered potential solutions to this issue:
   -aspectbench compiler. Alternative to AspectJ's ajc compiler. Initially attractive due to promises of optimizations (faster instrumented code) and extensible nature already including extensions for array element access pointcuts. Ended up simply being too old and buggy to actually use.
   -Spring AOP. Too limited, does not even support set/get pointcuts without using AspectJ.
   -JBoss AOP. A newer AOP implementation for Java. Attractive due to relatively recent maintenance (last release Jan 2010), and documented support for array element access pointcuts. Currently working on porting my tracker to this. The process of weaving aspects into libraries is less automated than it is for aspectJ. and relies on the use of an ant task. I had to learn ant to carefully manage the weaving, and now have a script which will do everything quite easily. Currently getting my aspects to work in the new environment.
   -Modify AspectJ to add support. Haven't seriously considered this. Seems like it would require too much time to understand the AspectJ source to the level where I can safely make this change.
   -Ignore array element access. Again I haven't seriously considered this. I can understand leaving some things out of the tracker (like data in primitives), but it seems to be that missing this would fragment the tracking too severely.

-------------



